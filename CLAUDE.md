# Claude Model Comparison

Comparative behavioral analysis of Claude model pairs using Claude Code session logs.

## Project Structure

```
scripts/                         # Reusable analysis pipeline
replication/                     # Controlled replication tasks
comparisons/<model-a>-vs-<model-b>/
    data/                        # Session metadata, classified tasks, tokens (private)
    analysis/                    # Statistical results (mix of private/public)
    prompts/                     # Report generation prompts
    report/                      # Report source (template, terms, expansions)
      report.html                # Source template (hand-edited)
      terms.json                 # Glossary term definitions (mouseover tooltips)
      expansions/                # Cached HTML fragments (expandable detail blocks)
      manifest.json              # Section hashes for expansion invalidation
    dist/public/report.html      # Built report output (generated by build_report.py)
```

## Running a Comparison

Use the pipeline orchestrator to run all steps:

```sh
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data
```

Or select specific steps:

```sh
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data --steps extract,classify
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data --from stats
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data --force          # ignore staleness
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data --check-stale    # show stale steps
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data --check-consistency  # verify counts
```

The pipeline tracks input/output file hashes in `data/pipeline-manifest.json`. Steps are skipped when inputs are unchanged since the last run. Use `--force` to override.

Steps (in order): `collect`, `extract`, `classify`, `annotate`, `analyze`, `tokens`, `stats`, `dataset`, `update`, `report`.

Individual scripts can also be run directly with `--data-dir` and `--analysis-dir` arguments:

```sh
python scripts/collect_sessions.py --data-dir comparisons/opus-4.5-vs-4.6/data
python scripts/extract_tasks.py --data-dir comparisons/opus-4.5-vs-4.6/data --canonical
python scripts/classify_tasks.py --data-dir comparisons/opus-4.5-vs-4.6/data
python scripts/annotate_tasks.py --data-dir comparisons/opus-4.5-vs-4.6/data
python scripts/run_analyses.py --data-dir comparisons/opus-4.5-vs-4.6/data --analysis-dir comparisons/opus-4.5-vs-4.6/analysis
python scripts/extract_tokens.py --dir comparisons/opus-4.5-vs-4.6
python scripts/stat_tests.py --data-dir comparisons/opus-4.5-vs-4.6/data --analysis-dir comparisons/opus-4.5-vs-4.6/analysis
python scripts/analyze_dataset.py --data-dir comparisons/opus-4.5-vs-4.6/data --analysis-dir comparisons/opus-4.5-vs-4.6/analysis
python scripts/update_sections.py --dir comparisons/opus-4.5-vs-4.6                          # update tables + prose
python scripts/update_sections.py --dir comparisons/opus-4.5-vs-4.6 --tables-only            # tables only, no LLM
python scripts/update_sections.py --dir comparisons/opus-4.5-vs-4.6 --sections cost --dry-run # preview changes
python scripts/build_report.py --dir comparisons/opus-4.5-vs-4.6
python scripts/build_report.py --dir comparisons/opus-4.5-vs-4.6 --check-stale  # check for outdated expansions
```

## Adding a New Comparison

1. Create `comparisons/<model-a>-vs-<model-b>/` with `data/`, `analysis/`, `prompts/`, `dist/public/` subdirectories
2. Collect sessions for each model into `data/sessions-<model>.json`
3. Run the pipeline scripts above
4. Customize `prompts/report-generation.md` for the specific findings
5. Generate the report

## Report Build System

The report uses a source template (`report/report.html`) that gets built into the final output (`dist/public/report.html`). The build step (`build_report.py`) adds:

- **Term tooltips**: First occurrences of glossary terms (defined in `report/terms.json`) get wrapped with CSS-only mouseover tooltips. No JavaScript required.
- **Expansion blocks**: `<!-- expand: name -->` markers in the template get replaced with `<details>` elements containing content from `report/expansions/{name}.html`. Each fragment can specify a title via `<!-- title: ... -->` on its first line.
- **Invalidation**: `report/manifest.json` tracks section content hashes. Run `--check-stale` to see which expansions need updating after template edits.

- **Template variables**: `{{section.metric_name}}` syntax in the template gets resolved from `report/variables.json`, which maps variable names to data paths in analysis JSON files.
- **Stale variable check**: Run `--check-stale-vars` to verify all `{{...}}` markers are resolved in the built output.

Edit the template at `report/report.html`, not the built output at `dist/public/report.html`.

## Section Update System

The `update` pipeline step (`scripts/update_sections.py`) auto-generates expansion tables from analysis data and LLM-checks prose against current numbers. It runs in two phases:

1. **Table generation**: Regenerate all tables from analysis data. Tables in both expansions and the main template use `<!-- GENERATED-TABLE: table-id -->` named markers.
2. **Template variable resolution**: Replace `{{section.metric}}` markers with data values from `report/variables.json`.
3. **Whole-document prose check**: Build an annotated template (inlining all expansions between `BEGIN-EXPANSION`/`END-EXPANSION` sentinels), send it with all key metrics in a single LLM call, then decompose the result back into template + expansion files.

The single-pass approach gives the LLM cross-section context and reduces SDK overhead vs per-section calls. `GENERATED-TABLE` markers tell the LLM which content is data-driven and should not be modified.

### Spec files

Each `report/specs/{section-id}.json` defines:
- **`data_sources`**: analysis JSON files to load
- **`tables`**: expansion table definitions (columns, row order, data paths)
- **`prose`**: key metrics paths and guidance for LLM fact-checking

Column paths use `{model_a}`, `{model_b}`, `{row_key}` placeholders. Display names use `{display_a}`, `{display_b}`.

### Table generation

`scripts/table_gen.py` is a pure computation module (no LLM). It generates HTML tables from spec + data. Complex tables (edit overlaps, stat tests) use custom row generators in `update_sections.py`.

### Prose caching

A single cache key (hash of annotated template + all metrics) is used. Second runs are no-ops when data hasn't changed. Cache is stored in `report/.prose-cache/`.

## Shared Utilities

- **`scripts/models.py`**: Model discovery (`discover_models()`, `discover_model_pair()`) and config parsing (`load_comparison_config()`). Eliminates hardcoded model lists.
- **`scripts/session_utils.py`**: Canonical JSONL parsing (`iter_messages()`, `iter_tasks()`, `extract_user_text()`). Single task boundary definition used by all scripts.

## Canonical Data Flow

```
JSONL sessions → tasks-canonical-{model}.json → tasks-annotated-{model}.json → analysis JSONs → report
```

No script after `extract_tasks.py` opens a JSONL file. All downstream analysis reads from canonical or annotated task files.

## Privacy

The `data/` directory contains session metadata referencing local file paths and user prompts. These are gitignored. Aggregated analysis files (stat-tests.json, token-analysis.json, etc.) contain only statistical summaries and are safe to commit.

## Dependencies

- Python 3.11+
- scipy (for stat_tests.py)
- Claude Code SDK (for LLM classification steps)
