# Claude Model Comparison

Comparative behavioral analysis of Claude model pairs using Claude Code session logs.

## Project Structure

```
scripts/                         # Reusable analysis pipeline
replication/                     # Controlled replication tasks
comparisons/<model-a>-vs-<model-b>/
    data/                        # Session metadata, classified tasks, tokens (private)
    analysis/                    # Statistical results (mix of private/public)
    prompts/                     # Report generation prompts
    report/                      # Report source (template, terms, expansions)
      report.html                # Source template (hand-edited)
      terms.json                 # Glossary term definitions (mouseover tooltips)
      expansions/                # Cached HTML fragments (expandable detail blocks)
      manifest.json              # Section hashes for expansion invalidation
    dist/public/report.html      # Built report output (generated by build_report.py)
```

## Running a Comparison

Use the pipeline orchestrator to run all steps:

```sh
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data
```

Or select specific steps:

```sh
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data --steps extract,classify
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data --from stats
```

Steps (in order): `collect`, `extract`, `classify`, `analyze`, `tokens`, `stats`, `edits`, `planning`, `compaction`, `update`, `report`.

Individual scripts can also be run directly with `--data-dir` and `--analysis-dir` arguments:

```sh
python scripts/collect_sessions.py --data-dir comparisons/opus-4.5-vs-4.6/data
python scripts/extract_tasks.py --data-dir comparisons/opus-4.5-vs-4.6/data
python scripts/classify_tasks.py --data-dir comparisons/opus-4.5-vs-4.6/data
python scripts/analyze_behavior.py --data-dir comparisons/opus-4.5-vs-4.6/data
python scripts/extract_tokens.py --dir comparisons/opus-4.5-vs-4.6
python scripts/stat_tests.py --data-dir comparisons/opus-4.5-vs-4.6/data --analysis-dir comparisons/opus-4.5-vs-4.6/analysis
python scripts/analyze_edits.py --data-dir comparisons/opus-4.5-vs-4.6/data --analysis-dir comparisons/opus-4.5-vs-4.6/analysis
python scripts/planning_analysis.py --data-dir comparisons/opus-4.5-vs-4.6/data --analysis-dir comparisons/opus-4.5-vs-4.6/analysis
python scripts/analyze_compaction.py --data-dir comparisons/opus-4.5-vs-4.6/data --analysis-dir comparisons/opus-4.5-vs-4.6/analysis
python scripts/update_sections.py --dir comparisons/opus-4.5-vs-4.6                          # update tables + prose
python scripts/update_sections.py --dir comparisons/opus-4.5-vs-4.6 --tables-only            # tables only, no LLM
python scripts/update_sections.py --dir comparisons/opus-4.5-vs-4.6 --sections cost --dry-run # preview changes
python scripts/build_report.py --dir comparisons/opus-4.5-vs-4.6
python scripts/build_report.py --dir comparisons/opus-4.5-vs-4.6 --check-stale  # check for outdated expansions
```

## Adding a New Comparison

1. Create `comparisons/<model-a>-vs-<model-b>/` with `data/`, `analysis/`, `prompts/`, `dist/public/` subdirectories
2. Collect sessions for each model into `data/sessions-<model>.json`
3. Run the pipeline scripts above
4. Customize `prompts/report-generation.md` for the specific findings
5. Generate the report

## Report Build System

The report uses a source template (`report/report.html`) that gets built into the final output (`dist/public/report.html`). The build step (`build_report.py`) adds:

- **Term tooltips**: First occurrences of glossary terms (defined in `report/terms.json`) get wrapped with CSS-only mouseover tooltips. No JavaScript required.
- **Expansion blocks**: `<!-- expand: name -->` markers in the template get replaced with `<details>` elements containing content from `report/expansions/{name}.html`. Each fragment can specify a title via `<!-- title: ... -->` on its first line.
- **Invalidation**: `report/manifest.json` tracks section content hashes. Run `--check-stale` to see which expansions need updating after template edits.

Edit the template at `report/report.html`, not the built output at `dist/public/report.html`.

## Section Update System

The `update` pipeline step (`scripts/update_sections.py`) auto-generates expansion tables from analysis data and LLM-checks prose against current numbers. It is driven by per-section spec files in `report/specs/`.

### Spec files

Each `report/specs/{section-id}.json` defines:
- **`data_sources`**: analysis JSON files to load
- **`tables`**: expansion table definitions (columns, row order, data paths)
- **`prose`**: key metrics paths and guidance for LLM fact-checking

Column paths use `{model_a}`, `{model_b}`, `{row_key}` placeholders. Display names use `{display_a}`, `{display_b}`.

### Table generation

`scripts/table_gen.py` is a pure computation module (no LLM). It generates HTML tables from spec + data. Complex tables (edit overlaps, stat tests) use custom row generators in `update_sections.py`.

### Prose caching

LLM results are cached in `report/.prose-cache/` keyed on content+data hash. Second runs are no-ops when data hasn't changed.

## Privacy

The `data/` directory contains session metadata referencing local file paths and user prompts. These are gitignored. Aggregated analysis files (stat-tests.json, token-analysis.json, etc.) contain only statistical summaries and are safe to commit.

## Dependencies

- Python 3.11+
- scipy (for stat_tests.py)
- Claude Code SDK (for LLM classification steps)
