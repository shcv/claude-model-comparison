#+TITLE: Claude Model Comparison

Comparative behavioral analysis of Claude model pairs using Claude Code session logs.

* Opus 4.5 vs Opus 4.6

The first comparison examines Claude Opus 4.5 (~claude-opus-4-5-20251101~) and
Opus 4.6 (~claude-opus-4-6~) across 2,395 tasks from real-world Claude Code usage.

[[https://shcv.github.io/claude-model-comparison/opus-4.5-vs-4.6/][Read the report]] ([[file:comparisons/opus-4.5-vs-4.6/dist/public/report.html][local]]).

Key findings:
- Users correct Opus 4.5 edits *16× more often* (8.9% vs 0.6%)
- Opus 4.6 is 9–45% cheaper per task despite producing 2.7× more output tokens
- Of 105 statistical tests with Bonferroni correction, only 3 survive — all about /how/ each model works, not /whether/ they succeed

* Project Structure

#+begin_example
scripts/                         # Reusable analysis pipeline
replication/                     # Controlled replication tasks
comparisons/<model-a>-vs-<model-b>/
    data/                        # Session metadata, classified tasks, tokens (private)
    analysis/                    # Statistical results (JSON)
    prompts/                     # Report generation prompts
    report/                      # Report source (template, terms, expansions)
      report.html                # Source template (hand-edited)
      terms.json                 # Glossary term definitions (mouseover tooltips)
      expansions/                # HTML fragments for expandable detail blocks
      specs/                     # Per-section data specs for auto-update
      manifest.json              # Section hashes for expansion invalidation
    dist/public/report.html      # Built report output (generated by build_report.py)
#+end_example

* Running the Pipeline

Use the orchestrator to run all steps:

#+begin_src bash
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data
#+end_src

Or select specific steps:

#+begin_src bash
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data --steps extract,classify
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data --from stats
#+end_src

Steps (in order): =collect=, =extract=, =classify=, =analyze=, =tokens=, =stats=, =edits=, =planning=, =compaction=, =update=, =report=.

* Section Update System

The =update= pipeline step auto-generates expansion tables from analysis data and
LLM-checks prose against current numbers. It is driven by per-section spec files
in =report/specs/=.

#+begin_src bash
# Regenerate all tables + check prose (full update)
python scripts/update_sections.py --dir comparisons/opus-4.5-vs-4.6

# Tables only, no LLM calls
python scripts/update_sections.py --dir comparisons/opus-4.5-vs-4.6 --tables-only

# Single section, dry run
python scripts/update_sections.py --dir comparisons/opus-4.5-vs-4.6 --sections cost --dry-run
#+end_src

Each spec file (=report/specs/{section-id}.json=) defines:
- *data_sources*: analysis JSON files to load
- *tables*: expansion table definitions (columns, row order, data paths)
- *prose*: key metrics paths and guidance for LLM fact-checking

Tables are generated by =scripts/table_gen.py= (pure computation, no LLM). Complex
tables use custom row generators in =update_sections.py=. LLM prose results are
cached in =report/.prose-cache/= — second runs are no-ops when data hasn't changed.

* Report Build System

The report uses a source template (~report/report.html~) that gets built into
the final output (~dist/public/report.html~). The build step adds:

- *Term tooltips*: First occurrences of glossary terms get CSS-only mouseover tooltips
- *Expansion blocks*: =<!-- expand: name -->= markers become =<details>= elements
- *Invalidation*: =manifest.json= tracks section hashes; run =--check-stale= to detect outdated expansions

Edit the template at =report/report.html=, not the built output.

#+begin_src bash
python scripts/build_report.py --dir comparisons/opus-4.5-vs-4.6
python scripts/build_report.py --dir comparisons/opus-4.5-vs-4.6 --check-stale
#+end_src

* Privacy

The =data/= directory contains session metadata referencing local file paths and
user prompts. These are gitignored. Aggregated analysis files contain only
statistical summaries and are safe to commit.

* Dependencies

- Python 3.11+
- scipy
- Claude Code SDK (for LLM classification and prose checking)
