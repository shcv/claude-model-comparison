#+TITLE: Claude Model Comparison

Comparative behavioral analysis of Claude model pairs using Claude Code session logs.

* Opus 4.5 vs Opus 4.6

The first comparison examines Claude Opus 4.5 (~claude-opus-4-5-20251101~) and
Opus 4.6 (~claude-opus-4-6~) across 2,320 tasks from real-world Claude Code usage.

[[https://shcv.github.io/claude-model-comparison/opus-4.5-vs-4.6/][Read the report]] ([[file:comparisons/opus-4.5-vs-4.6/dist/public/report.html][local]]).

Key findings:
- Opus 4.6 rewrites its own edits 10.3% of the time vs 16.6% for 4.5 — a 38% reduction
- Opus 4.6 is 9–35% cheaper per task at trivial through moderate complexity despite producing 2.4× more output tokens
- Of 529 statistical tests, 15 survive Bonferroni correction at the overall level — most describe /how/ each model works, not /whether/ it succeeds

Every number in the report is computed directly from analysis data by deterministic Python scripts. LLMs assist with task classification and prose drafting, not fact production.

* Project Structure

#+begin_example
scripts/                         # Reusable analysis pipeline
replication/                     # Controlled replication tasks
comparisons/<model-a>-vs-<model-b>/
    data/                        # Session metadata, classified tasks, tokens (private)
    analysis/                    # Statistical results (JSON)
    prompts/                     # Report generation prompts
    report/                      # Report source (template, terms, expansions)
      report.html                # Source template (hand-edited)
      variables.json             # Variable definitions mapping names to analysis JSON paths
      terms.json                 # Glossary term definitions (mouseover tooltips)
      specs/                     # Per-section data specs for auto-update
      expansions/                # HTML fragments for expandable detail blocks
      manifest.json              # Section hashes for expansion invalidation
    dist/public/report.html      # Built report output (generated by build_report.py)
#+end_example

* Running the Pipeline

Use the orchestrator to run all steps:

#+begin_src bash
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data
#+end_src

Or select specific steps:

#+begin_src bash
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data --steps extract,classify
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data --from stats
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data --force          # ignore staleness
python scripts/run_pipeline.py --data-dir comparisons/opus-4.5-vs-4.6/data --no-llm         # skip LLM steps
#+end_src

Steps (in order): =collect=, =extract=, =classify=, =annotate=, =analyze=, =tokens=, =enrich=, =stats=, =findings=, =dataset=, =update=, =report=.

The pipeline tracks input/output file hashes in =data/pipeline-manifest.json=. Steps are skipped when inputs are unchanged since the last run. Use =--force= to override. Use =--no-llm= to skip LLM-dependent steps (=annotate=) and run =update= in tables-only mode.

* Section Update System

The =update= pipeline step auto-generates expansion tables from analysis data and
resolves data-bound expressions in prose. It runs in three phases:

1. *Table generation*: Regenerate all tables from analysis data using =<!-- GENERATED-TABLE: id -->= markers.
2. *Expression authoring* (LLM): Convert literal numbers in prose to =={{expression | format}}== markers. Caches results; becomes a no-op once all literals are converted.
3. *Expression resolution*: Evaluate all expressions via =scripts/expr_eval.py= (safe AST-based evaluator). Supports lookups, math, and builtins (=abs=, =round=, =min=, =max=).

#+begin_src bash
# Regenerate all tables + author expressions (full update)
python scripts/update_sections.py --dir comparisons/opus-4.5-vs-4.6

# Tables only, no LLM calls
python scripts/update_sections.py --dir comparisons/opus-4.5-vs-4.6 --tables-only

# Single section, dry run
python scripts/update_sections.py --dir comparisons/opus-4.5-vs-4.6 --sections cost --dry-run
#+end_src

Each spec file (=report/specs/{section-id}.json=) defines:
- *data_sources*: analysis JSON files to load
- *tables*: expansion table definitions (columns, row order, data paths)
- *prose*: key metrics paths and guidance for LLM expression authoring

Tables are generated by =scripts/table_gen.py= (pure computation, no LLM). Complex
tables use custom row generators in =update_sections.py=. Expression authoring results
are cached in =report/.prose-cache/= — second runs are no-ops when data hasn't changed.

Expressions are wrapped in sentinels: =<!-- var: expr -->value<!-- /var -->=. The build
step strips sentinels; the update step re-resolves them.

* Report Build System

The report uses a source template (~report/report.html~) that gets built into
the final output (~dist/public/report.html~). The build step adds:

- *Term tooltips*: First occurrences of glossary terms get CSS-only mouseover tooltips
- *Expansion blocks*: =<!-- expand: name -->= markers become =<details>= elements
- *Template variables*: =={{section.metric}}== syntax resolves from =variables.json=
- *Invalidation*: =manifest.json= tracks section hashes; run =--check-stale= to detect outdated expansions

Edit the template at =report/report.html=, not the built output.

#+begin_src bash
python scripts/build_report.py --dir comparisons/opus-4.5-vs-4.6
python scripts/build_report.py --dir comparisons/opus-4.5-vs-4.6 --check-stale
python scripts/build_report.py --dir comparisons/opus-4.5-vs-4.6 --check-stale-vars
#+end_src

* Privacy

The =data/= directory contains session metadata referencing local file paths and
user prompts. These are gitignored. Aggregated analysis files contain only
statistical summaries and are safe to commit.

* Dependencies

- Python 3.11+
- scipy
- Claude Code SDK (for LLM classification and expression authoring)
