#+TITLE: Model Comparison: Opus 4.5 vs Opus 4.6

Observational comparison of Claude Opus 4.5 (~claude-opus-4-5-20251101~) and
Opus 4.6 (~claude-opus-4-6~) from real-world Claude Code usage.

* Quick Start

Read =report.org= for the unified findings.

* Directory Structure

#+begin_example
model-comparison/
  report.org              # Unified findings (start here)
  analysis-process.org    # Methodology documentation
  README.org              # This file
  .gitignore
  data/                   # Raw and processed JSON data
    README.org            # Data dictionary
    sessions-*.json       # Session metadata
    tasks-*.json          # Task lists
    tasks-deep-*.json     # Full task data with tool calls
    tasks-classified-*.json  # Tasks with complexity labels
  analysis/               # Analysis outputs
    llm-analysis-*.json   # LLM qualitative judgments
    behavior-metrics.json # Subagent/planning metrics
    behavior-report.org   # Behavioral analysis summary
    comparison-report.md  # Corrected dissatisfaction analysis
    llm-comparison-report.org  # LLM judgment comparison
    planning-parallelization-report.org  # Deep-dive on planning/subagents
    scores-latest.json    # Sampled task scoring
    dissatisfaction-audit.json  # False positive audit
  scripts/                # Analysis pipeline
    collect_sessions.py   # Step 1: Extract sessions from JSONL
    extract_tasks.py      # Step 2: Deep task extraction
    classify_tasks.py     # Step 3: Complexity classification
    analyze_tasks_llm.py  # Step 4: LLM qualitative analysis
    analyze_behavior.py   # Step 5: Behavioral metrics
    normalize_llm_fields.py  # Normalize LLM judgments
    analyze.py            # Orchestrator for scoring
    sanitize.py           # Data anonymization for publication
    browse_tasks.py       # Interactive task browser
    segment_tasks.py      # Session segmentation
    replicate_task.py     # Task replication setup
  prompts/                # LLM prompts used by scripts
  replication/            # Controlled replication experiments
    tasks/                # Task configs (JSON)
    prompts/              # Replication prompts (text)
    results/              # Completed replications
  dist/                   # (gitignored) Sanitized publication tiers
#+end_example

* Running Scripts

All scripts use relative paths (=data/=, =analysis/=) and should be run from
the project root:

#+begin_src bash
python scripts/collect_sessions.py --model opus-4-6 --output data/sessions-opus-4-6.json
python scripts/extract_tasks.py --data-dir data
python scripts/classify_tasks.py --data-dir data
python scripts/analyze_tasks_llm.py --workers 8 --report
python scripts/analyze_behavior.py --report
python scripts/normalize_llm_fields.py
python scripts/sanitize.py
#+end_src

* Data

See =data/README.org= for the data dictionary.

Two publication tiers are generated by =scripts/sanitize.py=:
- =dist/anthropic/=: Full data with personal paths anonymized
- =dist/public/=: Aggregate statistics only, no raw task data
